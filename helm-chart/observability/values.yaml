# Global settings
namespace: monitoring

# Grafana configuration
grafana:
  enabled: true
  adminUser: admin
  adminPassword: "ChangeMe-StrongPassword"

  persistence:
    enabled: true
    size: 5Gi

  resources:
    requests:
      cpu: 100m
      memory: 256Mi
    limits:
      memory: 1Gi

  datasources: {}
    # Note: datasources will need to be configured manually in Grafana UI
    # Prometheus: http://monitoring-kube-prometheus-stack-prometheus.monitoring.svc.cluster.local:9090
    # Loki: http://loki.monitoring.svc.cluster.local:3100
    # Tempo: http://tempo.monitoring.svc.cluster.local:3100

# Prometheus configuration (kube-prometheus-stack)
prometheus:
  enabled: true
  
  alertmanager:
    enabled: true
    persistentVolume:
      enabled: true
      size: 2Gi
    resources:
      requests:
        cpu: 50m
        memory: 128Mi
      limits:
        memory: 512Mi

  prometheus:
    retention: "7d"
    prometheusSpec:
      retention: "7d"
      storageSpec:
        volumeClaimTemplate:
          spec:
            accessModes: ["ReadWriteOnce"]
            resources:
              requests:
                storage: 20Gi
      resources:
        requests:
          cpu: 200m
          memory: 512Mi
        limits:
          memory: 2Gi
      serviceMonitorSelectorNilUsesHelmValues: false
      podMonitorSelectorNilUsesHelmValues: false
      ruleSelectorNilUsesHelmValues: false

  # On Raspberry, reducing scope helps a lot
  kubeStateMetrics:
    enabled: true
  nodeExporter:
    enabled: true

  prometheusOperator:
    resources:
      requests:
        cpu: 100m
        memory: 128Mi
      limits:
        memory: 256Mi

  # Global scrape config
  defaultRules:
    create: true
  global:
    scrape_interval: 60s
    evaluation_interval: 60s

  pushgateway:
    enabled: false

# Loki configuration
loki:
  enabled: true
  deploymentMode: SingleBinary

  loki:
    auth_enabled: false
    commonConfig:
      replication_factor: 1
    storage:
      type: filesystem
    schemaConfig:
      configs:
        - from: "2024-01-01"
          store: tsdb
          object_store: filesystem
          schema: v13
          index:
            prefix: loki_index_
            period: 24h

    limits_config:
      retention_period: 168h   # 7 days
      reject_old_samples: true
      reject_old_samples_max_age: 168h

  singleBinary:
    replicas: 1
    persistence:
      enabled: true
      size: 20Gi
    resources:
      requests:
        cpu: 100m
        memory: 256Mi
      limits:
        memory: 1Gi

  # Disable distributed modes (lighter for Raspberry)
  read:
    replicas: 0
  write:
    replicas: 0
  backend:
    replicas: 0

# Tempo configuration
tempo:
  enabled: true
  tempo:
    retention: 168h  # 7 days

  persistence:
    enabled: true
    size: 10Gi

  resources:
    requests:
      cpu: 100m
      memory: 256Mi
    limits:
      memory: 1Gi

  # Simple mode for homelab
  replicas: 1

# Alloy configuration
alloy:
  enabled: true
  alloy:
    configMap:
      create: true
      content: |
        logging {
          level = "info"
        }

        discovery.kubernetes "pods" {
          role = "pod"
        }

        loki.source.kubernetes "pod_logs" {
          targets    = discovery.kubernetes.pods.targets
          forward_to = [loki.write.loki.receiver]
        }

        loki.write "loki" {
          endpoint {
            url = "http://loki.monitoring.svc.cluster.local:3100/loki/api/v1/push"
          }
        }

        otelcol.receiver.otlp "otlp" {
          grpc { endpoint = "0.0.0.0:4317" }
          http { endpoint = "0.0.0.0:4318" }
          output {
            traces = [otelcol.exporter.otlp.tempo.input]
          }
        }

        otelcol.exporter.otlp "tempo" {
          client {
            endpoint = "tempo.monitoring.svc.cluster.local:4317"
            tls {
              insecure = true
            }
          }
        }

  controller:
    resources:
      requests:
        cpu: 100m
        memory: 256Mi
      limits:
        memory: 1Gi

  # Needs to run on nodes to read logs
  daemonset:
    enabled: true

# MetalLB configuration
# Note: MetalLB is deployed in the same namespace as the release (monitoring)
metallb:
  enabled: true

  # Local network configuration
  network:
    subnet: "192.168.1.0/24"
    # IP range for LoadBalancer services
    ipRange:
      start: "192.168.1.100"
      end: "192.168.1.250"

# Cloudflare Tunnel configuration
cloudflared:
  enabled: true
  # Cloudflare Tunnel namespace
  namespace: cloudflare

  # Cloudflare Tunnel token
  # Get your token from: https://one.dash.cloudflare.com/
  # The token contains all tunnel configuration including routes
  token: "YOUR_CLOUDFLARE_TUNNEL_TOKEN_HERE"
  
  # Image configuration
  image:
    repository: cloudflare/cloudflared
    tag: latest
    pullPolicy: IfNotPresent
  
  # Resources
  resources:
    requests:
      cpu: 50m
      memory: 64Mi
    limits:
      cpu: 100m
      memory: 128Mi

